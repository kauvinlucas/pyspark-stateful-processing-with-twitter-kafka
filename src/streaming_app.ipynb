{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56c888ba",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming - Event-Time and Stateful Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6338073",
   "metadata": {},
   "source": [
    "El Structured Streaming de Apache Spark es un motor de procesamiento de streaming scalable y tolerante a fallas en el motor de Spark SQL. Cuando definimos a una fuente de datos como streaming, Spark se encarga de hacer la ejecución de manera incremental y de actualizar el resultado a medida que los datos llegan a la fuente.\n",
    "\n",
    "Podemos procesar los datos en streaming de diversas maneras, tales como:\n",
    "* Hacer transformaciones a los datos y expresar agregaciones\n",
    "* Procesar los datos en ventanas de event time\n",
    "* Realizar joins stream-to-batch\n",
    "* Entre otros\n",
    "\n",
    "Por defecto, Spark procesa los datos en **micro-batches**, que son pequeños trabajos que se realizan de manera sequencial a medida que los datos llegan. Pero también es posible sacar provecho del **Continuous Streaming** (modo de procesamiento de baja latencia), sin sacrificar los transformaciones en los DataFrames/Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76c957c",
   "metadata": {},
   "source": [
    "#### Objetivo de este notebook\n",
    "\n",
    "Este notebook tiene por objetivo guiarnos en el API de Structured Streaming, para el procesamiento en base a eventos (event time processing) con un ejemplo sencillo de ingesta y transformación de *tweets* en Twitter mediante su API. Utilizaremos **Apache Kafka** como motor de almacenamiento en streaming distribuido para hacer la lectura de los datos.\n",
    "\n",
    "Nos enfocaremos en el modelo de procesamiento micro-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec49d51",
   "metadata": {},
   "source": [
    "## 1. Leyendo los datos del Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1126f139",
   "metadata": {},
   "source": [
    "#### Iniciar SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1250e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar la instalación de Spark\n",
    "import findspark\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "spark_installation = os.environ[\"SPARK_HOME\"]\n",
    "findspark.init(spark_installation)\n",
    "\n",
    "# Iniciar SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "spark = spark_session = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Twitter Kafka Stream\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1\")\\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Reducir el número de Shuffle partitions para 5\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661d8a38",
   "metadata": {},
   "source": [
    "#### Cargar Kafka como fuente de streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a609ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el tópico de Kafka\n",
    "topic_consumer = \"delatam-streaming\"\n",
    "\n",
    "# Hacer la lectura\n",
    "streaming_in = spark.readStream.format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    ".option(\"subscribe\", topic_consumer)\\\n",
    ".option(\"startingOffsets\", \"latest\")\\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8713d8b4",
   "metadata": {},
   "source": [
    "## Transformaciones y agregaciones en streaming\n",
    "\n",
    "Podemos emplear virtualmente todas las transformaciones que se realizan a los Dataframes y Datasets estáticos. Una gran ventaja del Structured Streaming es que se ejecuta en el mismo motor de Spark SQL, lo que nos posibilita expresar las transformaciones de la misma manera de cómo expresaríamos en un procesamiento en batch.\n",
    "\n",
    "Para ver la transformación en acción, es necesario ejecutar el `producer.py` para Kafka pase a recibir los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd566751",
   "metadata": {},
   "source": [
    "#### Convertir el valor al DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e153cb1",
   "metadata": {},
   "source": [
    "Definimos el *schema* para cargar los datos almacenados en la columna `value`. Es preferible no inferir el esquema de los datos para no crear jobs desnecesarios, principalmente si leemos archivos con grandes volúmenes de datos.\n",
    "\n",
    "Luego, hacemos la conversión del `creation_time`, cuyo tipo pasaría de *string* a *timestamp*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668ba945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as t\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "# Definir el esquema\n",
    "schema = t.StructType([\n",
    "    t.StructField(\"creation_time\", t.StringType(), True),\n",
    "    t.StructField(\"tweet_text\", t.StringType(), False)])\n",
    "\n",
    "# Seleccionar la columna de valor\n",
    "recent_tweets = streaming_in.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Transformar los datos con from_json\n",
    "recent_tweets = recent_tweets.select(f.from_json(f.col(\"value\"), schema).alias(\"data\"))\\\n",
    ".select(\"data.*\")\n",
    "\n",
    "# Convertir la fecha de creación a timestamp\n",
    "recent_tweets = recent_tweets.withColumn(\"creation_time\",f.to_timestamp(\"creation_time\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf625f4a",
   "metadata": {},
   "source": [
    "#### Aplicar una transformación de ejemplo (wordcount)\n",
    "\n",
    "Aplicamos una transformación de wordcount para contar las palabras que aparecen en cada tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcount = recent_tweets.withColumn('word', f.explode(f.split(f.col('tweet_text'), ' ')))\\\n",
    "    .groupBy('word')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afd4713",
   "metadata": {},
   "source": [
    "#### Escribir el output al console sink para prueba y definir la variable de consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15728b22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_console = wordcount \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0d0b72",
   "metadata": {},
   "source": [
    "#### Ver los streams activos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19dd94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eabdb06",
   "metadata": {},
   "source": [
    "#### Detener el stream utilizando la variable de consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72559c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_console.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5235b7",
   "metadata": {},
   "source": [
    "## Procesamiento de *event time* y *stateful processing*\n",
    "\n",
    "Spark Streaming nos permite realizar transformaciones a los datos que caen en un periodo tardío en el motor de streaming. En este caso, se toma el *event time* como atributo para el procesamiento. El *event time* está incorporado dentro de los datos (cada evento es un registro), y representa el periodo de tiempo en que se generó el registro o evento.\n",
    "\n",
    "Comúnmente se aplican funciones de ventana y  agregaciones para procesar los datos en intervalos de tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c27c2fb",
   "metadata": {},
   "source": [
    "#### Aplicar una transformación en base a *event time*\n",
    "\n",
    "Las transformaciones con *event time* funcionan de manera muy similar a las agregraciones con *groupBy*. De hecho, se aplica el *groupBy* y la función *window* para procesar los datos que caen dentro del rango del evento especificado.\n",
    "\n",
    "En el siguiente ejemplo, hacemos un conteo de palabras en cada tweet cuyo *event time* se encuentra en el rango o intervalo definido por *window*. El cálculo se hace para cada intervalo, razón por la cual un mismo registro puede ser calculado dentro de uno o más intervalos de *event time*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7977cccf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hacer la escritura\n",
    "query_event_time = recent_tweets\\\n",
    "    .groupBy(f.window(f.col(\"creation_time\"), \"5 minutes\", \"1 minutes\")).count()\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"tweets_per_window\")\\\n",
    "    .format(\"console\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84952f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detener el streaming\n",
    "query_event_time.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acad7bc",
   "metadata": {},
   "source": [
    "#### Aplicar una transformación en base a event time con *watermark*\n",
    "\n",
    "El *watermark* es un técnica que nos permite descartar los datos que llegan muy tardes al motor de motor de Streaming, en vez de procesarlos en base a una tabla intermedia en memória que sólo crece con el pasar del tiempo. \n",
    "\n",
    "En otras palabras, si un dato llega y su *timestamp* cae en el intervalo cuyo término con respecto a periodo máximo (más actual) es mayor al rango del *watermark*, Spark no procesaría ese dato y eliminaría el intervalo antiguo.\n",
    "\n",
    "En el siguiente ejemplo, los tweets más recientes se leen primero y son procesados. No obstante, a medida que Spark va leyendo los tweets más recientes, se va eliminando los intervalos más antiguos y los tweets que cairían en los rangos eliminados simplemente no se procesarían."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f5a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer la escritura con watermarking\n",
    "query_event_time = recent_tweets\\\n",
    "    .withWatermark(\"creation_time\", \"1 minutes\")\\\n",
    "    .groupBy(f.window(f.col(\"creation_time\"), \"5 minutes\", \"1 minutes\")).count()\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"tweets_per_window\")\\\n",
    "    .format(\"console\")\\\n",
    "    .outputMode(\"update\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2f583e",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "\n",
    "Spark Streaming nos ofrece una variedad de formas para procesar los datos en streaming. Una de estas formas es procesar los datos que pueden llegar tarde al motor de streaming mediante funciones de agregación con groupBy y con la función window para especificar los intervalos de tiempo para realizar el cálculo.\n",
    "\n",
    "Hemos visto también como Spark descarta los datos que llegan muy tardes al motor de procesamiento mediante el *watermarking*. El *watermarking* previene un gasto mayor de recursos  por mantener y procesar datos relacionados a un intervalo de tiempo que es muy lejano al periodo actual."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
